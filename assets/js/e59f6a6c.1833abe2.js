"use strict";(globalThis.webpackChunkwuair_docs=globalThis.webpackChunkwuair_docs||[]).push([[2146],{7535(e,t,i){i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"State-Estimation/state-estimation-research","title":"State Estimation Research","description":"This is where we will compile new possible methods and algorithms for the state estimation team. Feel free to add sources here. When you do please leave a short description of the method and how we can use it for WUAIR.","source":"@site/docs/State-Estimation/state-estimation-research.md","sourceDirName":"State-Estimation","slug":"/State-Estimation/state-estimation-research","permalink":"/wuair_docs/docs/State-Estimation/state-estimation-research","draft":false,"unlisted":false,"editUrl":"https://github.com/WU-AI-Racing/wuair_docs.git/docs/State-Estimation/state-estimation-research.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"State Estimation","permalink":"/wuair_docs/docs/category/state-estimation"},"next":{"title":"Path Planning","permalink":"/wuair_docs/docs/category/path-planning"}}');var a=i(4848),r=i(8453);const s={sidebar_position:1},o="State Estimation Research",l={},d=[{value:"General Overview",id:"general-overview",level:2},{value:"References",id:"references",level:2},{value:"1. Extended Kalman Filtering",id:"1-extended-kalman-filtering",level:3},{value:"2. Recurrent Kalman Network",id:"2-recurrent-kalman-network",level:3},{value:"3. Advanced Mapping Techniques",id:"3-advanced-mapping-techniques",level:3},{value:"4. Fast LiDAR Odometry (F-LOAM)",id:"4-fast-lidar-odometry-f-loam",level:3},{value:"5. Graph-SLAM in Racing",id:"5-graph-slam-in-racing",level:3},{value:"6. FastSLAM Implementation",id:"6-fastslam-implementation",level:3}];function h(e){const t={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"state-estimation-research",children:"State Estimation Research"})}),"\n",(0,a.jsx)(t.p,{children:"This is where we will compile new possible methods and algorithms for the state estimation team. Feel free to add sources here. When you do please leave a short description of the method and how we can use it for WUAIR."}),"\n",(0,a.jsx)(t.admonition,{title:"Adding New References",type:"tip",children:(0,a.jsx)(t.p,{children:"Please add any research you come across! Use the same formatting as below and please leave a short description of the source to make it easy to navigate"})}),"\n",(0,a.jsx)(t.h2,{id:"general-overview",children:"General Overview"}),"\n",(0,a.jsxs)(t.p,{children:["The Image below shows our place in the greater system diagram. Our job is to take in the positions of the cones and output a map of the track and the vehicles current position and trajectory. This first requires estimating our current velocity, which we will do by combining the data from our sensors using a Kalman Filtering. We will then use that velocity estimation in our SLAM implementation.\n",(0,a.jsx)(t.img,{alt:"System Diagram with SLAM Highlighted",src:i(349).A+"",title:"System Overview",width:"1582",height:"988"})]}),"\n",(0,a.jsx)(t.h2,{id:"references",children:"References"}),"\n",(0,a.jsx)(t.h3,{id:"1-extended-kalman-filtering",children:"1. Extended Kalman Filtering"}),"\n",(0,a.jsxs)(t.p,{children:["*",(0,a.jsx)(t.a,{href:"https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/",children:"https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/"}),"\n*[",(0,a.jsx)(t.a,{href:"https://automaticaddison.com/how-to-derive-the-observation-model-for-a-mobile-robot/%5D(",children:"https://automaticaddison.com/how-to-derive-the-observation-model-for-a-mobile-robot/]("}),(0,a.jsx)(t.a,{href:"https://automaticaddison.com/how-to-derive-the-observation-model-for-a-mobile-robot/",children:"https://automaticaddison.com/how-to-derive-the-observation-model-for-a-mobile-robot/"}),"\n*",(0,a.jsx)(t.a,{href:"https://automaticaddison.com/how-to-derive-the-state-space-model-for-a-mobile-robot/",children:"https://automaticaddison.com/how-to-derive-the-state-space-model-for-a-mobile-robot/"}),")"]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"These are some good articles to read to learn about Extended Kalman filters, which we may end up implementing"}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"2-recurrent-kalman-network",children:"2. Recurrent Kalman Network"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Becker, P., Pandya, H., Gebhardt, G., Zhao, C., Taylor, J., & Neumann, G. (2019)."}),"\n*Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces. arXiv preprint arXiv:1905.07357.  [",(0,a.jsx)(t.a,{href:"https://arxiv.org/pdf/1905.07357%5D(arXiv:1905.07357v1",children:"https://arxiv.org/pdf/1905.07357](arXiv:1905.07357v1"})," [cs.LG])"]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Overview:"})," This paper introduces Recurrent Kalman Networks (RKN), a deep-learning\u2013based state-estimation framework that blends neural network feature extraction with Kalman-filter\u2013style uncertainty tracking. The method learns a latent state space directly from high-dimensional, noisy observations (such as images) and performs factorized Kalman updates in that learned space. This hybrid design addresses limitations of traditional Kalman filters (which require known linear models) and recurrent neural networks (which lack principled uncertainty estimates), enabling robust sequential inference, prediction, and reconstruction in dynamic environments."]}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"3-advanced-mapping-techniques",children:"3. Advanced Mapping Techniques"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Liu, C., Zhang, G., Rong, Y., Shao, W., Meng, J., Li, G., & Huang, Y. (2023)."})," ",(0,a.jsx)(t.em,{children:"Hybrid metric-feature mapping based on camera and Lidar sensor fusion."})," Measurement, 207, 112411. ",(0,a.jsx)(t.a,{href:"https://doi.org/10.1016/j.measurement.2022.112411",children:"https://doi.org/10.1016/j.measurement.2022.112411"})]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Overview:"}),' This paper proposes a mapping method that combines "metric" information (precise distances from LiDAR) with "feature" information (visual descriptors from cameras). This hybrid approach aims to solve the shortcomings of using either sensor individually for SLAM (Simultaneous Localization and Mapping).']}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"4-fast-lidar-odometry-f-loam",children:"4. Fast LiDAR Odometry (F-LOAM)"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Wang, H., Wang, C., & Xie, L. (2021)."})," ",(0,a.jsx)(t.em,{children:"F-LOAM: Fast LiDAR Odometry and Mapping."})," 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). ",(0,a.jsx)(t.a,{href:"https://ieeexplore.ieee.org/document/9636655",children:"https://ieeexplore.ieee.org/document/9636655"})]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Overview:"})," This paper presents a general solution for LiDAR-based SLAM that prioritizes computational efficiency (running at >10Hz). It uses a non-iterative two-stage distortion compensation method. This is highly relevant for WUAIR to ensure our localization doesn't lag behind the physical car at high speeds."]}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"5-graph-slam-in-racing",children:"5. Graph-SLAM in Racing"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Alvarez, A., Denner, N., Feng, Z., et al. (2022)."})," ",(0,a.jsx)(t.em,{children:"The Software Stack That Won the Formula Student Driverless Competition."})," ArXiv. ",(0,a.jsx)(t.a,{href:"https://arxiv.org/pdf/2210.10933",children:"https://arxiv.org/pdf/2210.10933"})]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Overview:"})," This report describes the software stack used by a winning Formula Student Driverless team. It specifically details how they used ",(0,a.jsx)(t.strong,{children:"GraphSLAM"})," to map track cones with a root-mean-square error of less than 15 cm while driving at speeds over 70 kph. This is a primary reference for implementing our cone-mapping algorithms."]}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"6-fastslam-implementation",children:"6. FastSLAM Implementation"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Wu, Y. (2023)."})," ",(0,a.jsx)(t.em,{children:"FastSLAM: Grid-based FastSLAM1.0 and FastSLAM2.0 algorithms."})," GitHub Repository. ",(0,a.jsx)(t.a,{href:"https://github.com/yingkunwu/FastSLAM",children:"https://github.com/yingkunwu/FastSLAM"})]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Overview:"})," A clean Python implementation of the grid-based FastSLAM algorithms. While we will likely use C++ for the production car, this repository is excellent for simulating and understanding the underlying particle-filter logic used in packages like ",(0,a.jsx)(t.code,{children:"gmapping"}),"."]}),"\n"]})]})}function c(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},349(e,t,i){i.d(t,{A:()=>n});const n=i.p+"assets/images/SLAMHighlight_Sytem-ceac716e1a748dda71b56242a75133d7.png"},8453(e,t,i){i.d(t,{R:()=>s,x:()=>o});var n=i(6540);const a={},r=n.createContext(a);function s(e){const t=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);